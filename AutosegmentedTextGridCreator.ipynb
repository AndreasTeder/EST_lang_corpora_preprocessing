{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "AutosegmentedTextGridCreator.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wranpQkrhDv_"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import errno\n",
        "import glob\n",
        "import os.path\n",
        "import shutil\n",
        "import subprocess, shlex\n",
        "import multiprocessing\n",
        "from multiprocessing import Process\n",
        "import time\n",
        "import re\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWwJ0na9hDwB"
      },
      "source": [
        "#Hääle lugejate dataframe kirjutamine\n",
        "\n",
        "#Kuna iga kõnekorpus on erinev, siis on raske luua universaalset lahendust\n",
        "#Seega kui Teie kõnekorpuse ülesehitus erineb drastiliselt UT uudiskorpusest, siis peate meetodit muutma\n",
        "\n",
        "#Tähtis on koodiploki lõpuks luua Dataframe data, kus on autosegmenteerimist soovivate failide nimekiri\n",
        "#Vajalikud veerud on \"speaker\", kus on rääkija nimi, \"filename\", kus on helifaili asukoht ja \"sent\", kus on faili loetud lause\n",
        "\n",
        "\n",
        "#Eeldame, et eksisteerib mingi juurkaust aadressil fileAddressBase, kus on iga kõneleja kohta kaust, kus asuvad csv failid, kus on kirjas helifailide asukohad ja loetud laused\n",
        "#Samuti eeldame, et helifailid eksisteerivad mingis juurkaustas, kus on iga kõneleja kohta kaust, kus asuvad helifailid\n",
        "#Samuti eeldame, et lause .txt failid eksisteerivad mingis juurkaustas, kus on iga kõneleja kohta kaust, kus asuvad lausete .txt failid (iga lause kohta 1 fail)\n",
        "\n",
        "#NB! Lause all mõeldakse kogu helifailis etteloetud teksti. Kui helifailis loetakse ette mitu lauset, siis see loeb kui 1 lause.\n",
        "UTspeakers = ['Mari', 'Kalev', 'Albert', 'Vesta'] #UT uudiste kõnekorpuse lugejad\n",
        "EKIspeakers = [\"Kylli\",\"Meelis\"] #EKI kõnekorpuse lugejad\n",
        "\n",
        "fileAddressBase = '/home/ubuntu/JupyterFile/Sents/' #Juurkaust, kus asuvad lugejate kaustad, kus asuvad kõigi lausete csv failid\n",
        "fileAddressPostfix = '/sentences.csv' #csv faili nimi, kus on kirjas eeltöötlust vajavate helifailide asukohtad ja sisseloetud laused\n",
        "\n",
        "data = pd.DataFrame()\n",
        "\n",
        "for speaker in UTspeakers:\n",
        "    sentencesFile = fileAddressBase + speaker +  fileAddressPostfix\n",
        "    sentences = pd.read_csv(sentencesFile, sep='|', header=None)\n",
        "    sentences.columns = ['filename','sent']\n",
        "    sentences['speaker'] = speaker\n",
        "    \n",
        "    sentences[\"filename\"] = sentences[\"filename\"].map(lambda filename: \"UT-uudised-\" + speaker + '/' + filename)\n",
        "\n",
        "    \n",
        "    data = data.append(sentences)\n",
        "\n",
        "for speaker in EKIspeakers:\n",
        "    sentencesFile = fileAddressBase + speaker +  fileAddressPostfix\n",
        "    sentences = pd.read_csv(sentencesFile, sep='|', header=None)\n",
        "    sentences.columns = ['filename', 'sent']\n",
        "    sentences['speaker'] = speaker\n",
        "    \n",
        "    sentences[\"filename\"] = sentences[\"filename\"].map(lambda filename: \"EKI-ilukirjandus-\" + speaker + '/' + filename)\n",
        "    \n",
        "    data = data.append(sentences)\n",
        "\n",
        "#Edasi askeldame koopiaga, nõnda saab notebook lõpus olevat andmevalikut paremini kasutada\n",
        "combinedData = data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7qtd1DuhDwD"
      },
      "source": [
        "#https://reactgo.com/python-run-mutliple-commands/\n",
        "#Kasutusel autosegmenteerija rakendamisel, et luua TextGrid, kus on kirjas, millal on vaikus\n",
        "    #Vajalik, kuna tahame kasutada Shelli ja teha seda lõimedega\n",
        "def subprocess_cmd(command):\n",
        "    process = subprocess.Popen(command,stdout=subprocess.PIPE,stderr=subprocess.PIPE, shell=True)\n",
        "    ret = process.wait()\n",
        "    proc_stdout = process.communicate()\n",
        "    \n",
        "#https://stackoverflow.com/questions/273192/how-can-i-safely-create-a-nested-directory\n",
        "def ifDirNotExistCreate(outputPath):\n",
        "    if not os.path.exists(os.path.dirname(outputPath)):\n",
        "        try:\n",
        "            os.makedirs(os.path.dirname(outputPath))\n",
        "        except OSError as exc: # Võidusõidu vastane kaitse\n",
        "            if exc.errno != errno.EEXIST:\n",
        "                raise\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Hh8IzePdhDwD",
        "outputId": "7b5eacac-bd00-429b-ef0c-2bb6eef1fa2c"
      },
      "source": [
        "#Loome igale datas olevale andmereale vastava TextGrid faili kasutades autosegmenteerijat https://bark.phon.ioc.ee/autosegment2/,\n",
        "    # milles on muuhulgas kirjas, mis hetkel algavad ja lõppevad vaikused\n",
        "\n",
        "    \n",
        "    # https://stackoverflow.com/questions/7207309/how-to-run-functions-in-parallel\n",
        "    # https://testdriven.io/blog/developing-an-asynchronous-task-queue-in-python/\n",
        "start = time.time() #Mõõdame töö aega\n",
        "\n",
        "processes = 3 #Mida rohkem multithreadingut, seda suurem tõenäosus, et autosegmenteerijal tekib probleeme ja peab hiljem uuesti proovima\n",
        "              #3 tundus olevat kiiruse mõttes optimaalne\n",
        "proc = [] #Salvestame siia kuni processes arv lõime kausutavat protsessi\n",
        "          #Teisisõnu saame korraga luua TextGride processes arv helifailile\n",
        "\n",
        "#Jörgmised kaustad on juurkaustad, seega tavaliselt on neis alamkaustad, kus asuvad vajalikud failid\n",
        "#Kaust kust loetakse helifailid sisse\n",
        "audioFileBaseLoc = \".\"\n",
        "#Kaust kust loetakse lausete .txt failid (iga lause eraldi txt fail)\n",
        "sentenceFileBaseLoc = \"SentencesWithPreprocessingTextFiles\"\n",
        "#Kaust kuhu kirjutatakse autosegmenteerija loodud TextGridid\n",
        "outputFileBaseLoc = \"TextGridsWithPreprocessedSents\"\n",
        "\n",
        "#Autosegmenteerija serveri aadress\n",
        "autosegmentAddress = \"http://10.15.0.2:8888/run\"\n",
        "\n",
        "#Võtab ühe töö järjekorrast ja laseb sellest TextGrid luua\n",
        "def curlOneFileFromQueueToAutosegment(task_queue):\n",
        "    while not task_queue.empty():\n",
        "        if(task_queue.qsize() % 50 == 0):\n",
        "            print(\"Files left to multithread originally:\",task_queue.qsize())\n",
        "        filename = task_queue.get()\n",
        "        curlOneFileToAutosegment(filename)\n",
        "    return True\n",
        "\n",
        "#Loob vastava failinimega audiofailile TextGridi\n",
        "def curlOneFileToAutosegment(filename):\n",
        "    baseFilename = (\".\").join(filename.split(\".\")[0:-1])\n",
        "    ifDirNotExistCreate(outputFileBaseLoc +\"/\"+baseFilename+'.TextGrid')\n",
        "    subprocess_cmd('curl --fail  --insecure  -F \"wav=@'+\n",
        "                    audioFileBaseLoc + \"/\" + baseFilename+'.wav\" -F \"txt=@'+\n",
        "                    sentenceFileBaseLoc + \"/\" + baseFilename+'.txt\" '+ autosegmentAddress + ' > '+\n",
        "                    outputFileBaseLoc +'/'+ baseFilename+'.TextGrid')\n",
        "    \n",
        "\n",
        "task_queue = multiprocessing.Queue()\n",
        "listOfFileNames = combinedData[\"filename\"].tolist() #Itereerime listi peal, seega konverteerime\n",
        "\n",
        "#Paneme iga helifaili tööde järjekorda\n",
        "for fileName in listOfFileNames:\n",
        "    task_queue.put(fileName)\n",
        "        \n",
        "for n in range(processes): #Limiteerime lõimede arvu processes muutujaga\n",
        "    p = Process(target=curlOneFileFromQueueToAutosegment, args=(task_queue,))\n",
        "    proc.append(p)\n",
        "    p.start()\n",
        "\n",
        "for p in proc:\n",
        "    p.join()\n",
        "    \n",
        "#Kuna eelmine TextGrid loomine ei tööta 10-20% juhtudel threading tõttu, käime kõik uuesti üle, ilma threadingutta\n",
        "#Ebaõnnestumise korral on loodud TextGrid suuruseks 0, seega saab niimoodi kontrollida failid üle\n",
        "#Kui TextGrid on tühi, siis proovime uuesti luua (Kui probleemiks oli mingi sümbol, siis loomulikult on ka siin tulemuseks tühi textgrid)\n",
        "\n",
        "#Statistika kogumiseks\n",
        "\n",
        "failedFileCounter = 0\n",
        "totalFileCounter = 0\n",
        "maxPossibleFileAmount = len(listOfFileNames)\n",
        "\n",
        "for filename in listOfFileNames:\n",
        "    baseFilename = (\".\").join(filename.split(\".\")[0:-1])\n",
        "    if os.path.getsize(outputFileBaseLoc + '/'+ baseFilename+\".TextGrid\") == 0:\n",
        "        curlOneFileToAutosegment(filename)\n",
        "        failedFileCounter += 1\n",
        "    totalFileCounter += 1\n",
        "    if(totalFileCounter % 50 == 0):\n",
        "        print(\"Have checked a total of\",totalFileCounter, \"files out of\", maxPossibleFileAmount)\n",
        "    \n",
        "print(\"Fail rate = \",failedFileCounter/totalFileCounter)\n",
        "print(\"Time taken = \",time.time() - start, \"s\")\n",
        "print(\"Total files = \", totalFileCounter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files left to multithread originally: 50\n",
            "Have checked a total of 50 files out of 60\n",
            "Fail rate =  0.06666666666666667\n",
            "Time taken =  94.31578540802002 s\n",
            "Total files =  60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U89iUzZbhDwF"
      },
      "source": [
        "#Kui on soov testimise eesmärgil valida vaid osa andmeid\n",
        "\n",
        "# limiit = 10\n",
        "\n",
        "# VestaData= (data[data[\"speaker\"]==\"Vesta\"])\n",
        "# VestaData= VestaData[0:limiit]\n",
        "\n",
        "# AlbertData= (data[data[\"speaker\"]==\"Albert\"])\n",
        "# AlbertData= AlbertData[0:limiit]\n",
        "\n",
        "# KalevData= (data[data[\"speaker\"]==\"Kalev\"])\n",
        "# KalevData= KalevData[0:limiit]\n",
        "\n",
        "# MariData= (data[data[\"speaker\"]==\"Mari\"])\n",
        "# MariData= MariData[0:limiit]\n",
        "\n",
        "# KylliData= (data[data[\"speaker\"]==\"Kylli\"])\n",
        "# KylliData= KylliData[0:limiit]\n",
        "\n",
        "# MeelisData= (data[data[\"speaker\"]==\"Meelis\"])\n",
        "# MeelisData= MeelisData[0:limiit]\n",
        "\n",
        "\n",
        "# combinedData = VestaData.append([AlbertData,KalevData,MariData,KylliData,MeelisData])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M26jsqoBhDwF",
        "outputId": "52766e86-15af-4fa8-f99e-dd4649f537f1"
      },
      "source": [
        "#Kui on soov testimise eesmärgil valida vaid osa andmeid, suvaliselt\n",
        "\n",
        "# limiit = 5\n",
        "\n",
        "# VestaData= shuffle(data[data[\"speaker\"]==\"Vesta\"])\n",
        "# VestaData.reset_index(inplace=True, drop=True)\n",
        "# VestaData= VestaData[0:limiit]\n",
        "\n",
        "# AlbertData= shuffle(data[data[\"speaker\"]==\"Albert\"])\n",
        "# AlbertData.reset_index(inplace=True, drop=True)\n",
        "# AlbertData= AlbertData[0:limiit]\n",
        "\n",
        "# KalevData= shuffle(data[data[\"speaker\"]==\"Kalev\"])\n",
        "# KalevData.reset_index(inplace=True, drop=True)\n",
        "# KalevData= KalevData[0:limiit]\n",
        "\n",
        "# MariData= shuffle(data[data[\"speaker\"]==\"Mari\"])\n",
        "# MariData.reset_index(inplace=True, drop=True)\n",
        "# MariData= MariData[0:limiit]\n",
        "\n",
        "# KylliData= shuffle(data[data[\"speaker\"]==\"Kylli\"])\n",
        "# KylliData.reset_index(inplace=True, drop=True)\n",
        "# KylliData= KylliData[0:limiit]\n",
        "\n",
        "# MeelisData= shuffle(data[data[\"speaker\"]==\"Meelis\"])\n",
        "# MeelisData.reset_index(inplace=True, drop=True)\n",
        "# MeelisData= MeelisData[0:limiit]\n",
        "\n",
        "# combinedData = VestaData.append([AlbertData,KalevData,MariData,KylliData,MeelisData])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}